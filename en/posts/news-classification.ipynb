{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 新闻文本分类\n\n- 学习链接：https://github.com/datawhalechina/team-learning-nlp/tree/master/NewsTextClassification\n- 比赛链接：[零基础入门NLP - 新闻文本分类 - 天池](https://tianchi.aliyun.com/competition/entrance/531810/introduction)"},{"metadata":{},"cell_type":"markdown","source":"## Task1\n\nhttps://tianchi.aliyun.com/notebook-ai/detail?spm=5176.12586969.1002.6.6406111aIKCSLV&postId=118252\n\n任务一主要是理解赛题和数据，并没实际工作量。\n\n数据下载解压之后，得到三个文件：\n\n```\ntest_a.csv               211M\ntest_a_sample_submit.csv 98K\ntrain_set.csv            840M\n```\n\n训练集和测试集都对字符进行了匿名处理，所以不用分词这一步。\n\n训练数据有 20w 条，使用 `\\t` 分隔，第一列为标签，第二列为文本。标签有 14 类，其对应关系为：\n\n```\n科技: 0\n股票: 1\n体育: 2\n娱乐: 3\n时政: 4\n社会: 5\n教育: 6\n财经: 7\n家居: 8\n游戏: 9\n房产: 10\n时尚: 11\n彩票: 12\n星座: 13\n```"},{"metadata":{},"cell_type":"markdown","source":"## Task2\n\nhttps://tianchi.aliyun.com/notebook-ai/detail?spm=5176.12586969.1002.9.6406111aIKCSLV&postId=118253\n\n任务二需要完成以下作业：\n\n1. 假设字符 3750，字符 900 和字符 648 是句子的标点符号，请分析赛题每篇新闻平均由多少个句子构成？\n2. 统计每类新闻中出现次数对多的字符\n\n### 文本长度"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import pandas as pd\n\n\ntrain_df = pd.read_csv('/kaggle/input/train_set.csv', sep='\\t')\n\n\ntrain_df['text_len'] = train_df['text'].apply(lambda x: len(x.split()))\ntrain_df['text_len'].describe()","execution_count":1,"outputs":[{"output_type":"execute_result","execution_count":1,"data":{"text/plain":"count    200000.000000\nmean        907.207110\nstd         996.029036\nmin           2.000000\n25%         374.000000\n50%         676.000000\n75%        1131.000000\nmax       57921.000000\nName: text_len, dtype: float64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"可以看出，平均每篇新闻有 907 个字符，最短的有 2 个字符，最长的有 57921 个字符。\n\n### 新闻类别分布\n\n统计每类新闻的样本个数。样本分布不平均，最少的只有 908 个，最多的达到了 38918 个。"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_df['label'].value_counts()","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"0     38918\n1     36945\n2     31425\n3     22133\n4     15016\n5     12232\n6      9985\n7      8841\n8      7847\n9      5878\n10     4920\n11     3131\n12     1821\n13      908\nName: label, dtype: int64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Q1\n\n对于问题一，需要把每篇文章的句子按标点符号切分后再计算句子个数，可直接正则模块切分。"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import re\n\n\ndef sentence_mean():\n    train_df['sentences'] = train_df['text'].apply(lambda x: len([s for s in re.split(r'\\b(?:3750|900|648)\\b', x) if s]))\n    return train_df['sentences'].mean()\n\n\nsentence_mean()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"78.922815"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"可以看出，每篇新闻平均有 79 个句子。\n\n### Q2\n\n问题二需要先根据新闻类别分类后再统一数据，这里使用到了 `loc` 对列数据进行筛选。"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from collections import Counter\n\n\ndef max_char():\n    res = []\n    for label in range(14):\n        c = Counter()\n        df = train_df['text'].loc[train_df['label']==label].apply(lambda x: x.split())\n        for news in df:\n            c.update(Counter(news))\n        res.append({label: c.most_common(1)[0]})\n        print('新闻类别 {} 出现最多的字符为 {}，共出现 {} 次'.format(label, *c.most_common(1)[0]))\n    return res\n\n\nmax_char()","execution_count":4,"outputs":[{"output_type":"stream","text":"新闻类别 0 出现最多的字符为 3750，共出现 1267331 次\n新闻类别 1 出现最多的字符为 3750，共出现 1200686 次\n新闻类别 2 出现最多的字符为 3750，共出现 1458331 次\n新闻类别 3 出现最多的字符为 3750，共出现 774668 次\n新闻类别 4 出现最多的字符为 3750，共出现 360839 次\n新闻类别 5 出现最多的字符为 3750，共出现 715740 次\n新闻类别 6 出现最多的字符为 3750，共出现 469540 次\n新闻类别 7 出现最多的字符为 3750，共出现 428638 次\n新闻类别 8 出现最多的字符为 3750，共出现 242367 次\n新闻类别 9 出现最多的字符为 3750，共出现 178783 次\n新闻类别 10 出现最多的字符为 3750，共出现 180259 次\n新闻类别 11 出现最多的字符为 3750，共出现 83834 次\n新闻类别 12 出现最多的字符为 3750，共出现 87412 次\n新闻类别 13 出现最多的字符为 3750，共出现 33796 次\n","name":"stdout"},{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"[{0: ('3750', 1267331)},\n {1: ('3750', 1200686)},\n {2: ('3750', 1458331)},\n {3: ('3750', 774668)},\n {4: ('3750', 360839)},\n {5: ('3750', 715740)},\n {6: ('3750', 469540)},\n {7: ('3750', 428638)},\n {8: ('3750', 242367)},\n {9: ('3750', 178783)},\n {10: ('3750', 180259)},\n {11: ('3750', 83834)},\n {12: ('3750', 87412)},\n {13: ('3750', 33796)}]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Task3\n\nhttps://tianchi.aliyun.com/notebook-ai/detail?spm=5176.12586969.1002.12.6406111aIKCSLV&postId=118254\n\n任务三需要完成以下作业：\n\n1. 尝试改变TF-IDF的参数，并验证精度\n2. 尝试使用其他机器学习模型，完成训练和验证\n\n在做作业之前，先根据文档学习一遍。首先是文本的表示方法\n\n### 文本的表示方法\n\n在数据真正进入训练之前，我们需要将原始文本转化为数字或向量。在词向量出现之前，看下有哪些方法。\n\n#### One-hot\n\n> 这里的One-hot与数据挖掘任务中的操作是一致的，即将每一个单词使用一个离散的向量表示。具体将每个字/词编码一个索引，然后根据索引进行赋值。\n\n概念还是比较容易理解的，即把语料库的词汇表中的每一个词都对应一个向量，向量的维度即为词汇表中词的个数，且向量中只有一位为 1，其余均为 0。\n\n把例子用代码来展示一下。\n\n参考 [sklearn.preprocessing.OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\ncorpus = [\n    *'我 爱 北 京 天 安 门'.split(),\n    *'我 喜 欢 上 海'.split()\n]\n\n\nlabel_enc = LabelEncoder()\nonehot_enc = OneHotEncoder(sparse=False)\n\nlabel_encoded = label_enc.fit_transform(corpus)\n\nonehot_enc.fit_transform(label_encoded.reshape(len(label_encoded), 1))","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"array([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### Bag of Words\n\n> Bag of Words（词袋表示），也称为Count Vectors，每个文档的字/词可以使用其出现次数来进行表示。\n\n将示例代码运行看看结果。"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n\ncorpus = [\n    'This is the first document.',\n    'This document is the second document.',\n    'And this is the third one.',\n    'Is this the first document?',\n]\n\nvectorizer = CountVectorizer()\nvectorizer.fit_transform(corpus).toarray()","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n       [0, 2, 0, 1, 0, 1, 1, 0, 1],\n       [1, 0, 0, 1, 1, 0, 1, 1, 1],\n       [0, 1, 1, 1, 0, 0, 1, 0, 1]])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### N-gram\n\n> N-gram 与 Count Vectors 类似，不过加入了相邻单词组合成为新的单词，并进行计数。\n\n代码展示与 Bag of Words 相似，只需修改下 `CountVectorizer` 的参数。\n\n指定的两个参数中，`analyzer` 指定词或字符级别的 n-grams，这里我们使用 `char`，`ngram_range` 指定 n-grams 的上下界。\n\n参考 [sklearn.feature_extraction.text.CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = [\n    '我爱北京天安门',\n    '我喜欢上海'\n]\n\n\nngram_vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 2))\nngram_vectorizer.fit_transform(corpus)\nngram_vectorizer.get_feature_names()","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"['上海', '京天', '北京', '喜欢', '天安', '安门', '我喜', '我爱', '欢上', '爱北']"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### tf-idf\n\n> tf-idf（英语：term frequency–inverse document frequency）是一种用于信息检索与文本挖掘的常用加权技术。tf-idf是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。\n\n词频：\n\n$$tf(t, d) = \\frac{词 t 在文档 d 中出现的次数}{文档 d 的总次数}$$\n\n逆文档频率：\n\n$$idf(t, D) = \\log{\\frac{N}{\\left| \\left\\{ d \\in D : t \\in d \\right\\} \\right|}}$$\n\n$N$ 表示文档数，$\\left| \\left\\{ d \\in D : t \\in d \\right\\} \\right|$ 表示包含词 t 的文档数。\n\n而 tf-idf 则是两者的乘积：\n\n$$tfidf(t, d, D) = tf(t, d) \\cdot idf(t, D)$$\n\n### 基于机器学习的文本分类\n\n看下第一个例子，使用 `CountVectorizer` 表示文本，使用岭回归分类器进行分类：\n\n```python\nimport pandas as pd\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.metrics import f1_score\n\n\n# 读取数据\ntrain_df = pd.read_csv('../data/train_set.csv', sep='\\t', nrows=15000)\n\n# 初始化词嵌入并载入数据进行学习\nvectorizer = CountVectorizer(max_features=3000)\ntrain_test = vectorizer.fit_transform(train_df['text'])\n\n# 初始化分类器并对前一万条数据（训练集）进行训练\nclf = RidgeClassifier()\nclf.fit(train_test[:10000], train_df['label'].values[:10000])\n\n# 对后 5000 条数据（测试集）进行预测并打分\nval_pred = clf.predict(train_test[10000:])\nprint(f1_score(train_df['label'].values[10000:], val_pred, average='macro'))\n```\n\n运行得到的结果是 `0.7422037924439758`。\n\n第二个例子使用 tf-idf 表示文本，同样使用岭回归分类器。主要代码改变如下，由 `CountVectorizer` 换为 `TfidfVectorizer`：\n\n```python\ntfidf = TfidfVectorizer(ngram_range=(1,3), max_features=3000)\n```\n\n得到的结果为 `0.8721598830546126`。"},{"metadata":{},"cell_type":"markdown","source":"### Q1\n\n在改变 tf-idf 的参数前，我们可以借助 `help` 函数看下 `TfidfVectorizer` 有哪些参数，或直接看 [sklearn.feature_extraction.text.TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html?highlight=tfidfvectorizer)。\n\n首先介绍几个主要相关的参数：\n\n- `ngram_rane`: `n-grams` 的上下界\n- `max_features`: 指定构建只包含词频出现前 `max_features` 次的词汇表\n\n试下改变上面的一个或两个参数，看下结果。"},{"metadata":{"trusted":true},"cell_type":"code","source":"import functools\n\n\nimport pandas as pd\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.metrics import f1_score\n\n\ntrain_df = pd.read_csv('/kaggle/input/train_set.csv', sep='\\t', nrows=15000)\n\n\n@functools.lru_cache()\ndef get_score(ngram_range=(1, 3), max_features=3000):\n    tfidf = TfidfVectorizer(ngram_range=ngram_range, max_features=max_features)\n    train_test = tfidf.fit_transform(train_df['text'])\n\n    clf = RidgeClassifier()\n    clf.fit(train_test[:10000], train_df['label'].values[:10000])\n\n    val_pred = clf.predict(train_test[10000:])\n    score = f1_score(train_df['label'].values[10000:], val_pred, average='macro')\n    print(f'{ngram_range}-{max_features}: {score}')\n    return score","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import operator\n\n\nridge_scores = {}\nfor a in ((1, 3), (1, 4), (2, 4), (1, 5), (2, 5)):\n    for b in (3000, 4000, 5000):\n        ridge_scores[f'{a}-{b}'] = get_score(a, b)\nprint(max(ridge_scores.items(), key=operator.itemgetter(1)))","execution_count":9,"outputs":[{"output_type":"stream","text":"(1, 3)-3000: 0.8721598830546126\n(1, 3)-4000: 0.8753945850878357\n(1, 3)-5000: 0.8850817067811825\n(1, 4)-3000: 0.8738210287555335\n(1, 4)-4000: 0.8747722106348722\n(1, 4)-5000: 0.8849155025217313\n(2, 4)-3000: 0.8217618443321154\n(2, 4)-4000: 0.8470765370291219\n(2, 4)-5000: 0.8668244198070033\n(1, 5)-3000: 0.8753002643279153\n(1, 5)-4000: 0.8751515660504635\n(1, 5)-5000: 0.8853177666563177\n(2, 5)-3000: 0.8257346646054335\n(2, 5)-4000: 0.8495083501418871\n(2, 5)-5000: 0.8658129484753834\n('(1, 5)-5000', 0.8853177666563177)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"就目前微调的几个参数来看，效果最好的组合是 `ngram_range=(1, 5), max_features=5000`，得分为 `0.8853177666563177`。\n\n### Q2\n\n这里使用 SVM、决策树等模型进行训练与验证，模型参数保持默认。"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n\n\n@functools.lru_cache()\ndef get_clf_score(clf, clf_name):\n    tfidf = TfidfVectorizer(ngram_range=(1, 3), max_features=3000)\n    train_test = tfidf.fit_transform(train_df['text'])\n\n    clf.fit(train_test[:10000], train_df['label'].values[:10000])\n\n    val_pred = clf.predict(train_test[10000:])\n    score = f1_score(train_df['label'].values[10000:], val_pred, average='macro')\n    print(f'{clf_name}: {score}')\n    return score\n\n\nclfs = {\n    'svm': SVC(),\n    'decision_tree': DecisionTreeClassifier(),\n    'k_neighbor': KNeighborsClassifier(),\n    'random_forest': RandomForestClassifier(),\n    'adaboost':AdaBoostClassifier(),\n}","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_scores = {}\nfor clf_name, clf in clfs.items():\n    clf_scores[clf_name] = get_clf_score(clf, clf_name)\n\nprint(clf_scores)","execution_count":11,"outputs":[{"output_type":"stream","text":"svm: 0.8761851818808672\ndecision_tree: 0.6563317180457323\nk_neighbor: 0.8001871379089528\nrandom_forest: 0.7816845182199408\nadaboost: 0.26965129982931424\n{'svm': 0.8761851818808672, 'decision_tree': 0.6563317180457323, 'k_neighbor': 0.8001871379089528, 'random_forest': 0.7816845182199408, 'adaboost': 0.26965129982931424}\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}