{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 新闻文本分类\n\n- 学习链接：https://github.com/datawhalechina/team-learning-nlp/tree/master/NewsTextClassification\n- 比赛链接：[零基础入门NLP - 新闻文本分类 - 天池](https://tianchi.aliyun.com/competition/entrance/531810/introduction)"},{"metadata":{},"cell_type":"markdown","source":"## Task1\n\nhttps://tianchi.aliyun.com/notebook-ai/detail?spm=5176.12586969.1002.6.6406111aIKCSLV&postId=118252\n\n任务一主要是理解赛题和数据，并没实际工作量。\n\n数据下载解压之后，得到三个文件：\n\n```\ntest_a.csv               211M\ntest_a_sample_submit.csv 98K\ntrain_set.csv            840M\n```\n\n训练集和测试集都对字符进行了匿名处理，所以不用分词这一步。\n\n训练数据有 20w 条，使用 `\\t` 分隔，第一列为标签，第二列为文本。标签有 14 类，其对应关系为：\n\n```\n科技: 0\n股票: 1\n体育: 2\n娱乐: 3\n时政: 4\n社会: 5\n教育: 6\n财经: 7\n家居: 8\n游戏: 9\n房产: 10\n时尚: 11\n彩票: 12\n星座: 13\n```"},{"metadata":{},"cell_type":"markdown","source":"## Task2\n\nhttps://tianchi.aliyun.com/notebook-ai/detail?spm=5176.12586969.1002.9.6406111aIKCSLV&postId=118253\n\n任务二需要完成以下作业：\n\n1. 假设字符 3750，字符 900 和字符 648 是句子的标点符号，请分析赛题每篇新闻平均由多少个句子构成？\n2. 统计每类新闻中出现次数对多的字符\n\n### 文本长度"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import pandas as pd\n\n\ntrain_df = pd.read_csv('/kaggle/input/train_set.csv', sep='\\t')\n\n\ntrain_df['text_len'] = train_df['text'].apply(lambda x: len(x.split()))\ntrain_df['text_len'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"可以看出，平均每篇新闻有 907 个字符，最短的有 2 个字符，最长的有 57921 个字符。\n\n### 新闻类别分布\n\n统计每类新闻的样本个数。样本分布不平均，最少的只有 908 个，最多的达到了 38918 个。"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_df['label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Q1\n\n对于问题一，需要把每篇文章的句子按标点符号切分后再计算句子个数，可直接正则模块切分。"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import re\n\n\ndef sentence_mean():\n    train_df['sentences'] = train_df['text'].apply(lambda x: len([s for s in re.split(r'\\b(?:3750|900|648)\\b', x) if s]))\n    return train_df['sentences'].mean()\n\n\nsentence_mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"可以看出，每篇新闻平均有 79 个句子。\n\n### Q2\n\n问题二需要先根据新闻类别分类后再统一数据，这里使用到了 `loc` 对列数据进行筛选。"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from collections import Counter\n\n\ndef max_char():\n    res = []\n    for label in range(14):\n        c = Counter()\n        df = train_df['text'].loc[train_df['label']==label].apply(lambda x: x.split())\n        for news in df:\n            c.update(Counter(news))\n        res.append({label: c.most_common(1)[0]})\n        print('新闻类别 {} 出现最多的字符为 {}，共出现 {} 次'.format(label, *c.most_common(1)[0]))\n    return res\n\n\nmax_char()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Task3\n\nhttps://tianchi.aliyun.com/notebook-ai/detail?spm=5176.12586969.1002.12.6406111aIKCSLV&postId=118254\n\n任务三需要完成以下作业：\n\n1. 尝试改变TF-IDF的参数，并验证精度\n2. 尝试使用其他机器学习模型，完成训练和验证\n\n在做作业之前，先根据文档学习一遍。首先是文本的表示方法\n\n### 文本的表示方法\n\n在数据真正进入训练之前，我们需要将原始文本转化为数字或向量。在词向量出现之前，看下有哪些方法。\n\n#### One-hot\n\n> 这里的One-hot与数据挖掘任务中的操作是一致的，即将每一个单词使用一个离散的向量表示。具体将每个字/词编码一个索引，然后根据索引进行赋值。\n\n概念还是比较容易理解的，即把语料库的词汇表中的每一个词都对应一个向量，向量的维度即为词汇表中词的个数，且向量中只有一位为 1，其余均为 0。\n\n把例子用代码来展示一下。\n\n参考 [sklearn.preprocessing.OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\ncorpus = [\n    *'我 爱 北 京 天 安 门'.split(),\n    *'我 喜 欢 上 海'.split()\n]\n\n\nlabel_enc = LabelEncoder()\nonehot_enc = OneHotEncoder(sparse=False)\n\nlabel_encoded = label_enc.fit_transform(corpus)\n\nonehot_enc.fit_transform(label_encoded.reshape(len(label_encoded), 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Bag of Words\n\n> Bag of Words（词袋表示），也称为Count Vectors，每个文档的字/词可以使用其出现次数来进行表示。\n\n将示例代码运行看看结果。"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n\ncorpus = [\n    'This is the first document.',\n    'This document is the second document.',\n    'And this is the third one.',\n    'Is this the first document?',\n]\n\nvectorizer = CountVectorizer()\nvectorizer.fit_transform(corpus).toarray()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### N-gram\n\n> N-gram 与 Count Vectors 类似，不过加入了相邻单词组合成为新的单词，并进行计数。\n\n代码展示与 Bag of Words 相似，只需修改下 `CountVectorizer` 的参数。\n\n指定的两个参数中，`analyzer` 指定词或字符级别的 n-grams，这里我们使用 `char`，`ngram_range` 指定 n-grams 的上下界。\n\n参考 [sklearn.feature_extraction.text.CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = [\n    '我爱北京天安门',\n    '我喜欢上海'\n]\n\n\nngram_vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 2))\nngram_vectorizer.fit_transform(corpus)\nngram_vectorizer.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### tf-idf\n\n> tf-idf（英语：term frequency–inverse document frequency）是一种用于信息检索与文本挖掘的常用加权技术。tf-idf是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。\n\n词频：\n\n$$tf(t, d) = \\frac{词 t 在文档 d 中出现的次数}{文档 d 的总次数}$$\n\n逆文档频率：\n\n$$idf(t, D) = \\log{\\frac{N}{\\left| \\left\\{ d \\in D : t \\in d \\right\\} \\right|}}$$\n\n$N$ 表示文档数，$\\left| \\left\\{ d \\in D : t \\in d \\right\\} \\right|$ 表示包含词 t 的文档数。\n\n而 tf-idf 则是两者的乘积：\n\n$$tfidf(t, d, D) = tf(t, d) \\cdot idf(t, D)$$\n\n### 基于机器学习的文本分类\n\n看下第一个例子，使用 `CountVectorizer` 表示文本，使用岭回归分类器进行分类：\n\n```python\nimport pandas as pd\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.metrics import f1_score\n\n\n# 读取数据\ntrain_df = pd.read_csv('../data/train_set.csv', sep='\\t', nrows=15000)\n\n# 初始化词嵌入并载入数据进行学习\nvectorizer = CountVectorizer(max_features=3000)\ntrain_test = vectorizer.fit_transform(train_df['text'])\n\n# 初始化分类器并对前一万条数据（训练集）进行训练\nclf = RidgeClassifier()\nclf.fit(train_test[:10000], train_df['label'].values[:10000])\n\n# 对后 5000 条数据（测试集）进行预测并打分\nval_pred = clf.predict(train_test[10000:])\nprint(f1_score(train_df['label'].values[10000:], val_pred, average='macro'))\n```\n\n运行得到的结果是 `0.7422037924439758`。\n\n第二个例子使用 tf-idf 表示文本，同样使用岭回归分类器。主要代码改变如下，由 `CountVectorizer` 换为 `TfidfVectorizer`：\n\n```python\ntfidf = TfidfVectorizer(ngram_range=(1,3), max_features=3000)\n```\n\n得到的结果为 `0.8721598830546126`。"},{"metadata":{},"cell_type":"markdown","source":"### Q1\n\n在改变 tf-idf 的参数前，我们可以借助 `help` 函数看下 `TfidfVectorizer` 有哪些参数，或直接看 [sklearn.feature_extraction.text.TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html?highlight=tfidfvectorizer)。\n\n首先介绍几个主要相关的参数：\n\n- `ngram_rane`: `n-grams` 的上下界\n- `max_features`: 指定构建只包含词频出现前 `max_features` 次的词汇表\n\n试下改变上面的一个或两个参数，看下结果。"},{"metadata":{"trusted":true},"cell_type":"code","source":"import functools\n\n\nimport pandas as pd\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.metrics import f1_score\n\n\ntrain_df = pd.read_csv('/kaggle/input/train_set.csv', sep='\\t', nrows=15000)\n\n\n@functools.lru_cache()\ndef get_score(ngram_range=(1, 3), max_features=3000):\n    tfidf = TfidfVectorizer(ngram_range=ngram_range, max_features=max_features)\n    train_test = tfidf.fit_transform(train_df['text'])\n\n    clf = RidgeClassifier()\n    clf.fit(train_test[:10000], train_df['label'].values[:10000])\n\n    val_pred = clf.predict(train_test[10000:])\n    score = f1_score(train_df['label'].values[10000:], val_pred, average='macro')\n    print(f'{ngram_range}-{max_features}: {score}')\n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import operator\n\n\nridge_scores = {}\nfor a in ((1, 3), (1, 4), (2, 4), (1, 5), (2, 5)):\n    for b in (3000, 4000, 5000):\n        ridge_scores[f'{a}-{b}'] = get_score(a, b)\nprint(max(ridge_scores.items(), key=operator.itemgetter(1)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"就目前微调的几个参数来看，效果最好的组合是 `ngram_range=(1, 5), max_features=5000`，得分为 `0.8853177666563177`。\n\n### Q2\n\n这里使用 SVM、决策树等模型进行训练与验证，模型参数保持默认。"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n\n\n@functools.lru_cache()\ndef get_clf_score(clf, clf_name):\n    tfidf = TfidfVectorizer(ngram_range=(1, 3), max_features=3000)\n    train_test = tfidf.fit_transform(train_df['text'])\n\n    clf.fit(train_test[:10000], train_df['label'].values[:10000])\n\n    val_pred = clf.predict(train_test[10000:])\n    score = f1_score(train_df['label'].values[10000:], val_pred, average='macro')\n    print(f'{clf_name}: {score}')\n    return score\n\n\nclfs = {\n    'svm': SVC(),\n    'decision_tree': DecisionTreeClassifier(),\n    'k_neighbor': KNeighborsClassifier(),\n    'random_forest': RandomForestClassifier(),\n    'adaboost':AdaBoostClassifier(),\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_scores = {}\nfor clf_name, clf in clfs.items():\n    clf_scores[clf_name] = get_clf_score(clf, clf_name)\n\nprint(clf_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Task4\n\n[Task4 基于深度学习的文本分类 1](https://github.com/datawhalechina/team-learning-nlp/blob/master/NewsTextClassification/Task4%20%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB1.md)\n\n本次任务有两个作业：\n\n1. 阅读 fastText 的文档，尝试修改参数，得到更好的分数\n2. 基于验证集的结果调整超参数，使得模型性能更优\n\n这次我们学习 fastText 并对新闻分类。\n\n在之前的任务三中，我们采用 tdidf 表示文本并使用机器学习算法进行新闻文本分类。如教学资料所说，前一章节的文本表示方法，存在一定的问题：\n\n> 转换得到的向量维度很高，需要较长的训练实践；没有考虑单词与单词之间的关系，只是进行了统计。\n\nfastText 可以用于学习词向量、文本分类，适用于该任务。\n\n看了示例代码，代码结构与之前的训练代码差不多，但输入数据需要进行特殊处理，因为 fastText 的输入数据的标签需要有 `__label__` 前缀。"},{"metadata":{"trusted":true},"cell_type":"code","source":"import fasttext\nimport pandas as pd\nfrom sklearn.metrics import f1_score\n\n\n# 读取数据\ntrain_df = pd.read_csv('/kaggle/input/train_set.csv', sep='\\t', nrows=15000)\n# 将数据转为 fastText 需要的格式\ntrain_df['label_ft'] = '__label__' + train_df['label'].astype(str)\n# 保存数据到文件\ntrain_df[['text','label_ft']].iloc[:-5000].to_csv('train.csv', index=None, header=None, sep='\\t')\n\nmodel = fasttext.train_supervised('train.csv', lr=1.0, wordNgrams=2, \n                                  verbose=2, minCount=1, epoch=25, loss=\"hs\", thread=1)\n\nval_pred = [model.predict(x)[0][0].split('__')[-1] for x in train_df.iloc[-5000:]['text']]\nprint(f1_score(train_df['label'].values[-5000:].astype(str), val_pred, average='macro'))  # 0.8231153757515691","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Q1\n\n试下不同学习率下的得分。"},{"metadata":{"trusted":true},"cell_type":"code","source":"import functools\nimport operator\n\n\n@functools.lru_cache()\ndef train_with_lr(lr):\n    model = fasttext.train_supervised('train.csv', lr=lr, wordNgrams=2, \n                                      verbose=2, minCount=1, epoch=25, loss=\"hs\", thread=1)\n\n    val_pred = [model.predict(x)[0][0].split('__')[-1] for x in train_df.iloc[-5000:]['text']]\n    score = f1_score(train_df['label'].values[-5000:].astype(str), val_pred, average='macro')\n    print(lr, '-', score)\n    return score\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fast_scores = {}\n\nfor lr in range(1, 11):\n    lr = lr / 10\n    fast_scores[lr] = train_with_lr(lr)\nprint(fast_scores)\n\nmax(fast_scores.items(), key=operator.itemgetter(1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"为了复现结果，把 `thread` 设置为 1。最终结果是在其他参数不变时，学习率为 1 效果最好。\n\n### Q2\n\nfastText 可以根据验证集自动调整超参数，我们试试看。\n\n首先把数据且分一下。使用 `split` 命令即可，每个文件 1000 行。"},{"metadata":{"trusted":true},"cell_type":"code","source":"!wc -l train.csv\n!split -l 1000 -d -a 1 --additional-suffix=.csv train.csv train_ \n!ls -alh","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"这里采用十折交叉法进行验证。"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\n\ndef get_train_and_validation(k):\n    train_file, test_file = f'_train_{k}.csv', f'train_{k}.csv'\n    if not os.path.exists(train_file):\n        with open(train_file, 'w', encoding='utf-8') as tf:\n            fns = [fn for fn in os.listdir() if fn.startswith('train_{k}')]\n            for fn in fns:\n                with open(fn, encoding='utf-8') as f:\n                    tf.write(f.read())\n    return train_file, test_file","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, valid = get_train_and_validation(0)\n# model = fasttext.train_supervised(input=train, autotuneValidationFile=valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"然而似乎出了点问题，`train_supervised` 函数一运行服务端就挂了，只能放弃。"},{"metadata":{},"cell_type":"markdown","source":"## Task5\n\n[Task5 基于深度学习的文本分类 2](https://github.com/datawhalechina/team-learning-nlp/blob/master/NewsTextClassification/Task5%20%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB2.md)\n\n本次学习涉及 Word2Vec、TextCNN、TextRNN 和 HAN，相关知识点较多。\n\nWord2Vec 的基本思路是通过中心词预测上下文（Skip-grams）或通过上下文预测中心词（CBOW）。\n在此思想之上，又有层次 softmax（Hierarchical softmax）和负采样（Nagative sampling）对训练过程进行优化。\n\nTextCNN 和 TextRNN 分别使用 CNN（卷积神经网络）和 RNN（循环神经网络）进行文本特征抽取，可以用来解决文本分类问题。\n\n> Hierarchical Attention Network for Document Classification(HAN) 基于层级注意力，在单词和句子级别分别编码并基于注意力获得文档的表示，然后经过Softmax进行分类。\n\n这次作业如下：\n\n1. 尝试通过 Word2Vec 训练词向量\n2. 尝试使用 TextCNN、TextRNN 完成文本表示\n3. 尝试使用 HAN 进行文本分类\n\nTextCNN 和 TextRNN 的代码可参考 [graykode/nlp-tutorial](https://github.com/graykode/nlp-tutorial)。"},{"metadata":{},"cell_type":"markdown","source":"### Q1\n\nWord2Vec 可借助 `gensim` 来训练，输入数据不严谨地处理一下，代码比较简单。"},{"metadata":{"trusted":true},"cell_type":"code","source":"import logging\nimport pandas as pd\n\n\nfrom gensim.models.word2vec import Word2Vec\n\n\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\ntrain_df = pd.read_csv('/kaggle/input/train_set.csv', sep='\\t', nrows=15000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def line_sentences():\n    return train_df['text'].apply(lambda x: x.split())\n\n\n\ndef word2vec_model():\n    sentences = line_sentences()\n    return Word2Vec(sentences, hs=1, window=6, workers=8, size=100)\n\n\n# model = word2vec_model()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Q2\n\n借助 [TextCNN-Torch.py](https://github.com/graykode/nlp-tutorial/blob/master/2-1.TextCNN/TextCNN-Torch.py) 的代码，稍微修改一下。"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\ndtype = torch.FloatTensor\n\n# Text-CNN Parameter\nembedding_size = 2 # n-gram\nsequence_length = 500\nnum_classes = 14  # 0 or 1\nfilter_sizes = [2, 2, 2] # n-gram window\nnum_filters = 3\n\n\ndef get_sentences_and_labels(start=0, end=None, length=sequence_length):\n    sentences, labels = [], []\n    if not end:\n        end = len(train_df['text'])\n    for s, l in zip(train_df['text'][start:end], train_df['label'][start:end]):\n        if len(s.split()) > length:\n            sentences.append(' '.join(s.split()[:length]))\n            labels.append(l)\n    return sentences, labels\n\nsentences, labels = get_sentences_and_labels(0, -5000)\n\n\nword_list = \" \".join(sentences).split()\nword_list = list(set(word_list))\nword_dict = {w: i for i, w in enumerate(word_list)}\nvocab_size = len(word_dict)\n\n\nclass TextCNN(nn.Module):\n    def __init__(self):\n        super(TextCNN, self).__init__()\n\n        self.num_filters_total = num_filters * len(filter_sizes)\n        self.W = nn.Parameter(torch.empty(vocab_size, embedding_size).uniform_(-1, 1)).type(dtype)\n        self.Weight = nn.Parameter(torch.empty(self.num_filters_total, num_classes).uniform_(-1, 1)).type(dtype)\n        self.Bias = nn.Parameter(0.1 * torch.ones([num_classes])).type(dtype)\n\n    def forward(self, X):\n        embedded_chars = self.W[X] # [batch_size, sequence_length, sequence_length]\n        embedded_chars = embedded_chars.unsqueeze(1) # add channel(=1) [batch, channel(=1), sequence_length, embedding_size]\n\n        pooled_outputs = []\n        for filter_size in filter_sizes:\n            # conv : [input_channel(=1), output_channel(=3), (filter_height, filter_width), bias_option]\n            conv = nn.Conv2d(1, num_filters, (filter_size, embedding_size), bias=True)(embedded_chars)\n            h = F.relu(conv)\n            # mp : ((filter_height, filter_width))\n            mp = nn.MaxPool2d((sequence_length - filter_size + 1, 1))\n            # pooled : [batch_size(=6), output_height(=1), output_width(=1), output_channel(=3)]\n            pooled = mp(h).permute(0, 3, 2, 1)\n            pooled_outputs.append(pooled)\n\n        h_pool = torch.cat(pooled_outputs, len(filter_sizes)) # [batch_size(=6), output_height(=1), output_width(=1), output_channel(=3) * 3]\n        h_pool_flat = torch.reshape(h_pool, [-1, self.num_filters_total]) # [batch_size(=6), output_height * output_width * (output_channel * 3)]\n\n        model = torch.mm(h_pool_flat, self.Weight) + self.Bias # [batch_size, num_classes]\n        return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = TextCNN()\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n\n# inputs = sentences\ninputs = []\nfor sen in sentences:\n    inputs.append(np.asarray([word_dict[n] for n in sen.split()]))\n\ntargets = []\nfor out in labels:\n    targets.append(out) # To using Torch Softmax Loss function\n\ninput_batch = Variable(torch.LongTensor(inputs))\ntarget_batch = Variable(torch.LongTensor(targets))\n\n\n# Training\nfor epoch in range(50):\n    optimizer.zero_grad()\n    output = model(input_batch)\n\n    # output : [batch_size, num_classes], target_batch : [batch_size] (LongTensor, not one-hot)\n    loss = criterion(output, target_batch)\n    if (epoch + 1) % 10 == 0:\n        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n\n    loss.backward()\n    optimizer.step()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"由于训练速度太慢了，这里只做了简单演示，偏差非常大。"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_labels, tests = [], []\nfor sen, l in zip(*get_sentences_and_labels(start=-5000)):\n    try:\n        tests.append(np.asarray([word_dict[n] for n in sen.split()]))\n        test_labels.append(l)\n    except KeyError:\n        pass\ntest_batch = Variable(torch.LongTensor(tests))\n\n\nval_pred = model(test_batch).data.max(1, keepdim=True)[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\n\nf1_score(test_labels, val_pred, average='macro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Task6\n\n[Task6 基于深度学习的文本分类 3](https://github.com/datawhalechina/team-learning-nlp/blob/master/NewsTextClassification/Task6%20%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB3.md)\n\n这次学习 Transformer 的原理和 Bert 的使用。\n\n作业：\n\n1. 完成 Bert Pretrain 和 Finetune 的过程\n2. 阅读 Bert 官方文档，找到相关参数进行调参"},{"metadata":{},"cell_type":"markdown","source":"预训练数据可通过 [create_pretraining_data.py](https://github.com/google-research/bert/blob/master/create_pretraining_data.py) 处理得到，之后进行预训练与微调。"},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nimport re\nfrom random import *\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/train_set.csv', sep='\\t', nrows=1500)\nsentences = []\nfor s in train_df['text']:\n    sentences.append(' '.join(s.split()[:256]))","execution_count":50,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# BERT Parameters\nmaxlen = 700\nbatch_size = 6\nmax_pred = 5 # max tokens of prediction\nn_layers = 6\nn_heads = 12\nd_model = 768\nd_ff = 768*4 # 4*d_model, FeedForward dimension\nd_k = d_v = 64  # dimension of K(=Q), V\nn_segments = 2\n\n# text = (\n#     'Hello, how are you? I am Romeo.\\n'\n#     'Hello, Romeo My name is Juliet. Nice to meet you.\\n'\n#     'Nice meet you too. How are you today?\\n'\n#     'Great. My baseball team won the competition.\\n'\n#     'Oh Congratulations, Juliet\\n'\n#     'Thanks you Romeo'\n# )\n# sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n') # filter '.', ',', '?', '!'\nword_list = list(set(\" \".join(sentences).split()))\nword_dict = {'[PAD]' : 0, '[CLS]' : 1, '[SEP]' : 2, '[MASK]' : 3}\nfor i, w in enumerate(word_list):\n    word_dict[w] = i + 4\nnumber_dict = {i: w for i, w in enumerate(word_dict)}\nvocab_size = len(word_dict)\n\ntoken_list = list()\nfor sentence in sentences:\n    arr = [word_dict[s] for s in sentence.split()]\n    token_list.append(arr)","execution_count":51,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample IsNext and NotNext to be same in small batch size\ndef make_batch():\n    batch = []\n    positive = negative = 0\n    while positive != batch_size/2 or negative != batch_size/2:\n        tokens_a_index, tokens_b_index= randrange(len(sentences)), randrange(len(sentences)) # sample random index in sentences\n        tokens_a, tokens_b= token_list[tokens_a_index], token_list[tokens_b_index]\n        input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']]\n        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n\n        # MASK LM\n        n_pred =  min(max_pred, max(1, int(round(len(input_ids) * 0.15)))) # 15 % of tokens in one sentence\n        cand_maked_pos = [i for i, token in enumerate(input_ids)\n                          if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]\n        shuffle(cand_maked_pos)\n        masked_tokens, masked_pos = [], []\n        for pos in cand_maked_pos[:n_pred]:\n            masked_pos.append(pos)\n            masked_tokens.append(input_ids[pos])\n            if random() < 0.8:  # 80%\n                input_ids[pos] = word_dict['[MASK]'] # make mask\n            elif random() < 0.5:  # 10%\n                index = randint(0, vocab_size - 1) # random index in vocabulary\n                input_ids[pos] = word_dict[number_dict[index]] # replace\n\n        # Zero Paddings\n        n_pad = maxlen - len(input_ids)\n        input_ids.extend([0] * n_pad)\n        segment_ids.extend([0] * n_pad)\n\n        # Zero Padding (100% - 15%) tokens\n        if max_pred > n_pred:\n            n_pad = max_pred - n_pred\n            masked_tokens.extend([0] * n_pad)\n            masked_pos.extend([0] * n_pad)\n\n        if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n            positive += 1\n        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n            negative += 1\n    return batch\n# Proprecessing Finished\n\n\ndef get_attn_pad_mask(seq_q, seq_k):\n    batch_size, len_q = seq_q.size()\n    batch_size, len_k = seq_k.size()\n    # eq(zero) is PAD token\n    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k\n\n\ndef gelu(x):\n    \"Implementation of the gelu activation function by Hugging Face\"\n    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n","execution_count":52,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Embedding(nn.Module):\n    def __init__(self):\n        super(Embedding, self).__init__()\n        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n        self.pos_embed = nn.Embedding(maxlen, d_model)  # position embedding\n        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n        self.norm = nn.LayerNorm(d_model)\n\n    def forward(self, x, seg):\n        seq_len = x.size(1)\n        pos = torch.arange(seq_len, dtype=torch.long)\n        pos = pos.unsqueeze(0).expand_as(x)  # (seq_len,) -> (batch_size, seq_len)\n        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n        return self.norm(embedding)\n\nclass ScaledDotProductAttention(nn.Module):\n    def __init__(self):\n        super(ScaledDotProductAttention, self).__init__()\n\n    def forward(self, Q, K, V, attn_mask):\n        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n        attn = nn.Softmax(dim=-1)(scores)\n        context = torch.matmul(attn, V)\n        return context, attn\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self):\n        super(MultiHeadAttention, self).__init__()\n        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n        self.W_K = nn.Linear(d_model, d_k * n_heads)\n        self.W_V = nn.Linear(d_model, d_v * n_heads)\n    def forward(self, Q, K, V, attn_mask):\n        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n        residual, batch_size = Q, Q.size(0)\n        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n\n        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n\n        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n        output = nn.Linear(n_heads * d_v, d_model)(context)\n        return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]\n\nclass PoswiseFeedForwardNet(nn.Module):\n    def __init__(self):\n        super(PoswiseFeedForwardNet, self).__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n\n    def forward(self, x):\n        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n        return self.fc2(gelu(self.fc1(x)))\n\nclass EncoderLayer(nn.Module):\n    def __init__(self):\n        super(EncoderLayer, self).__init__()\n        self.enc_self_attn = MultiHeadAttention()\n        self.pos_ffn = PoswiseFeedForwardNet()\n\n    def forward(self, enc_inputs, enc_self_attn_mask):\n        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n        return enc_outputs, attn\n\nclass BERT(nn.Module):\n    def __init__(self):\n        super(BERT, self).__init__()\n        self.embedding = Embedding()\n        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n        self.fc = nn.Linear(d_model, d_model)\n        self.activ1 = nn.Tanh()\n        self.linear = nn.Linear(d_model, d_model)\n        self.activ2 = gelu\n        self.norm = nn.LayerNorm(d_model)\n        self.classifier = nn.Linear(d_model, 2)\n        # decoder is shared with embedding layer\n        embed_weight = self.embedding.tok_embed.weight\n        n_vocab, n_dim = embed_weight.size()\n        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n        self.decoder.weight = embed_weight\n        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n\n    def forward(self, input_ids, segment_ids, masked_pos):\n        output = self.embedding(input_ids, segment_ids)\n        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n        for layer in self.layers:\n            output, enc_self_attn = layer(output, enc_self_attn_mask)\n        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n        # it will be decided by first token(CLS)\n        h_pooled = self.activ1(self.fc(output[:, 0])) # [batch_size, d_model]\n        logits_clsf = self.classifier(h_pooled) # [batch_size, 2]\n\n        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n        # get masked position from final output of transformer.\n        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n        h_masked = self.norm(self.activ2(self.linear(h_masked)))\n        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n\n        return logits_lm, logits_clsf\n","execution_count":53,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = BERT()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nbatch = make_batch()\ninput_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n\nfor epoch in range(100):\n    optimizer.zero_grad()\n    logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n    loss_lm = (loss_lm.float()).mean()\n    loss_clsf = criterion(logits_clsf, isNext) # for sentence classification\n    loss = loss_lm + loss_clsf\n    if (epoch + 1) % 10 == 0:\n        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n    loss.backward()\n    optimizer.step()\n","execution_count":54,"outputs":[{"output_type":"stream","text":"Epoch: 0010 cost = 58.499546\nEpoch: 0020 cost = 49.788536\nEpoch: 0030 cost = 29.034573\nEpoch: 0040 cost = 22.535231\nEpoch: 0050 cost = 13.636372\nEpoch: 0060 cost = 9.926339\nEpoch: 0070 cost = 8.569845\nEpoch: 0080 cost = 7.444367\nEpoch: 0090 cost = 4.532728\nEpoch: 0100 cost = 5.288932\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"还需要慢慢学习。上面都是依葫芦画瓢，无参考价值。"},{"metadata":{},"cell_type":"markdown","source":"### 参考\n\n- [【NLP】Google BERT详解](https://zhuanlan.zhihu.com/p/46652512)\n- [google-research/bert](https://github.com/google-research/bert)\n- [graykode/nlp-tutorial - BERT-Torch](https://github.com/graykode/nlp-tutorial/blob/master/5-2.BERT/BERT-Torch.py)","attachments":{}}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}